{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import missingno as msno\n",
    "from datetime import *\n",
    "import warnings\n",
    "import calendar\n",
    "from scipy.stats import kurtosis, skew\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some default functions to ease the data exploration\n",
    "def int_min(param):\n",
    "    return np.iinfo(param).min\n",
    "\n",
    "def int_max(param):\n",
    "    return np.iinfo(param).max\n",
    "\n",
    "def flo_min(param):\n",
    "    return np.finfo(param).min\n",
    "\n",
    "def flo_max(param):\n",
    "    return np.finfo(param).max\n",
    "\n",
    "def decrease_mem(in_table, verbose = True):\n",
    "    data_types = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    initial_mem_val = in_table.memory_usage().sum() / 2**20 #Calculate memory usage of complete column \n",
    "                                                            #and dividng it by 2^20 to convert it into Mb    \n",
    "    for c in in_table.columns:\n",
    "        column_type = in_table[c].dtypes\n",
    "        if column_type in data_types:\n",
    "            min_val = in_table[c].min()\n",
    "            max_val = in_table[c].max()\n",
    "            if str(column_type)[:3] == 'int':\n",
    "                if min_val > int_min(np.int8) and max_val < int_max(np.int8): #calling another fn int_min & int_max\n",
    "                    in_table[c] = in_table[c].astype(np.int8)\n",
    "                elif min_val > int_min(np.int16) and max_val < int_max(np.int16):\n",
    "                    in_table[c] = in_table[c].astype(np.int16)\n",
    "                elif min_val > int_min(np.int32) and max_val < int_max(np.int32):\n",
    "                    in_table[c] = in_table[c].astype(np.int32)\n",
    "                elif min_val > int_min(np.int64) and max_val < int_max(np.int64):\n",
    "                    in_table[c] = in_table[c].astype(np.int64)  \n",
    "            elif min_val > flo_min(np.float16) and max_val < flo_max(np.float16):\n",
    "                    in_table[c] = in_table[c].astype(np.float16)\n",
    "            elif min_val > flo_min(np.float32) and max_val < flo_max(np.float32):\n",
    "                    in_table[c] = in_table[c].astype(np.float32)\n",
    "            else: in_table[c] = in_table[c].astype(np.float64)\n",
    "                \n",
    "    mem_aftr_ops = in_table.memory_usage().sum() / 2**20\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.\n",
    "          format(mem_aftr_ops, ((initial_mem_val - mem_aftr_ops) / initial_mem_val)* 100))\n",
    "    return in_table\n",
    "\n",
    "def EDA(df_table):\n",
    "    summ = pd.DataFrame(df_table.dtypes, columns=['Data Type'])\n",
    "    summ = summ.reset_index()\n",
    "    summ['Name'] = summ['index']\n",
    "    summ = summ[['Name', 'Data Type']]\n",
    "    summ['Missing Values'] = df_table.isnull().sum().values\n",
    "    summ['Unique'] = df_table.nunique().values\n",
    "    summ['First Value'] = df_table.loc[0].values\n",
    "    summ['Second Value'] = df_table.loc[1].values\n",
    "    summ['Third Value'] = df_table.loc[2].values\n",
    "    summ['Minimum Value'] = df_table.min().values\n",
    "    summ['Maximum Value'] = df_table.max().values\n",
    "    #summary['Uniques'] = df.nunique().values\n",
    "    \n",
    "    for name in summ['Name'].value_counts().index:\n",
    "        summ.loc[summ['Name'] == name, 'Entropy'] = round(stats.entropy(df_table[name].value_counts(normalize=True), \n",
    "                                                                        base=2),2)\n",
    "    return summ\n",
    "\n",
    "def plot_stack(column_1, column_2):\n",
    "    plot_stck=pd.crosstab(index=column_1, columns=column_2)\n",
    "    plot_stck.plot(kind='bar', figsize=(8,8), stacked=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>...</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>day</th>\n",
       "      <th>year</th>\n",
       "      <th>peak_time</th>\n",
       "      <th>weekoff_count</th>\n",
       "      <th>best_condition</th>\n",
       "      <th>not_fav</th>\n",
       "      <th>sine_hr</th>\n",
       "      <th>cos_hr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.40</td>\n",
       "      <td>81</td>\n",
       "      <td>6.986063</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.63</td>\n",
       "      <td>80</td>\n",
       "      <td>6.845595</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.63</td>\n",
       "      <td>80</td>\n",
       "      <td>6.845595</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.40</td>\n",
       "      <td>75</td>\n",
       "      <td>6.672821</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.40</td>\n",
       "      <td>75</td>\n",
       "      <td>6.672821</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  season  holiday  workingday  weather  temp  atemp  \\\n",
       "0  2011-01-01 00:00:00       1        0           0        1  9.84  14.40   \n",
       "1  2011-01-01 01:00:00       1        0           0        1  9.02  13.63   \n",
       "2  2011-01-01 02:00:00       1        0           0        1  9.02  13.63   \n",
       "3  2011-01-01 03:00:00       1        0           0        1  9.84  14.40   \n",
       "4  2011-01-01 04:00:00       1        0           0        1  9.84  14.40   \n",
       "\n",
       "   humidity  windspeed  casual  ...  month   weekday day  year  peak_time  \\\n",
       "0        81   6.986063       3  ...      1  Saturday   1  2011          0   \n",
       "1        80   6.845595       8  ...      1  Saturday   1  2011          0   \n",
       "2        80   6.845595       5  ...      1  Saturday   1  2011          0   \n",
       "3        75   6.672821       3  ...      1  Saturday   1  2011          0   \n",
       "4        75   6.672821       0  ...      1  Saturday   1  2011          0   \n",
       "\n",
       "  weekoff_count  best_condition  not_fav   sine_hr    cos_hr  \n",
       "0             0               0        0  0.000000  1.000000  \n",
       "1             0               0        0  0.258819  0.965926  \n",
       "2             0               0        0  0.500000  0.866025  \n",
       "3             0               0        0  0.707107  0.707107  \n",
       "4             0               0        0  0.866025  0.500000  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the dataset 'mod_data' for model training\n",
    "\n",
    "mod_data = pd.read_csv(r\"D:\\OneDrive - Queen's University\\ECE\\Statistical Learning\\Midterm\\mod_data.csv\")\n",
    "mod_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have tried to get an brief idea of what would be the best features for training the model. For that I have used sklearn feature selection library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - Feature selected: cos_hr\n",
      "['cos_hr']\n",
      "1 - Feature selected: sine_hr\n",
      "2 - Feature selected: cos_hr\n",
      "['sine_hr', 'cos_hr']\n",
      "1 - Feature selected: peak_time\n",
      "2 - Feature selected: sine_hr\n",
      "3 - Feature selected: cos_hr\n",
      "['peak_time', 'sine_hr', 'cos_hr']\n",
      "1 - Feature selected: temp\n",
      "2 - Feature selected: peak_time\n",
      "3 - Feature selected: sine_hr\n",
      "4 - Feature selected: cos_hr\n",
      "['temp', 'peak_time', 'sine_hr', 'cos_hr']\n",
      "1 - Feature selected: temp\n",
      "2 - Feature selected: humidity\n",
      "3 - Feature selected: peak_time\n",
      "4 - Feature selected: sine_hr\n",
      "5 - Feature selected: cos_hr\n",
      "['temp', 'humidity', 'peak_time', 'sine_hr', 'cos_hr']\n",
      "1 - Feature selected: temp\n",
      "2 - Feature selected: humidity\n",
      "3 - Feature selected: month\n",
      "4 - Feature selected: peak_time\n",
      "5 - Feature selected: sine_hr\n",
      "6 - Feature selected: cos_hr\n",
      "['temp', 'humidity', 'month', 'peak_time', 'sine_hr', 'cos_hr']\n",
      "1 - Feature selected: temp\n",
      "2 - Feature selected: humidity\n",
      "3 - Feature selected: month\n",
      "4 - Feature selected: peak_time\n",
      "5 - Feature selected: best_condition\n",
      "6 - Feature selected: sine_hr\n",
      "7 - Feature selected: cos_hr\n",
      "['temp', 'humidity', 'month', 'peak_time', 'best_condition', 'sine_hr', 'cos_hr']\n",
      "1 - Feature selected: season\n",
      "2 - Feature selected: temp\n",
      "3 - Feature selected: humidity\n",
      "4 - Feature selected: windspeed\n",
      "5 - Feature selected: month\n",
      "6 - Feature selected: peak_time\n",
      "7 - Feature selected: sine_hr\n",
      "8 - Feature selected: cos_hr\n",
      "['season', 'temp', 'humidity', 'windspeed', 'month', 'peak_time', 'sine_hr', 'cos_hr']\n",
      "1 - Feature selected: season\n",
      "2 - Feature selected: temp\n",
      "3 - Feature selected: humidity\n",
      "4 - Feature selected: month\n",
      "5 - Feature selected: peak_time\n",
      "6 - Feature selected: weekoff_count\n",
      "7 - Feature selected: best_condition\n",
      "8 - Feature selected: sine_hr\n",
      "9 - Feature selected: cos_hr\n",
      "['season', 'temp', 'humidity', 'month', 'peak_time', 'weekoff_count', 'best_condition', 'sine_hr', 'cos_hr']\n",
      "1 - Feature selected: season\n",
      "2 - Feature selected: temp\n",
      "3 - Feature selected: humidity\n",
      "4 - Feature selected: month\n",
      "5 - Feature selected: year\n",
      "6 - Feature selected: peak_time\n",
      "7 - Feature selected: weekoff_count\n",
      "8 - Feature selected: best_condition\n",
      "9 - Feature selected: sine_hr\n",
      "10 - Feature selected: cos_hr\n",
      "['season', 'temp', 'humidity', 'month', 'year', 'peak_time', 'weekoff_count', 'best_condition', 'sine_hr', 'cos_hr']\n"
     ]
    }
   ],
   "source": [
    "# Performing feature selection using Sklearn's feature selection library. Based on this, the best features are \n",
    "# ['season', 'temp', 'humidity', 'month', 'year', 'peak_time', 'weekoff_count', 'best_condition', 'sine_hr', 'cos_hr']\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "from dataframe_column_identifier import DataFrameColumnIdentifier\n",
    "dfci = DataFrameColumnIdentifier()\n",
    "\n",
    "fs_cols = ['season', 'holiday', 'workingday', 'weather', 'temp',\n",
    "           'humidity', 'windspeed', 'month', 'year', 'peak_time',\n",
    "           'weekoff_count', 'best_condition', 'not_fav', 'sine_hr', 'cos_hr']\n",
    "\n",
    "fs_label = np.array(mod_data['count'])\n",
    "\n",
    "for i in range(1, 11):\n",
    "    kbest = SelectKBest(score_func = mutual_info_regression, k = i)\n",
    "    X_clf_new = kbest.fit_transform(mod_data[fs_cols], fs_label)\n",
    "    kbest_get_support_output = kbest.get_support()\n",
    "\n",
    "    print(dfci.select_columns_KBest(one_data[fs_cols], kbest_get_support_output, verbose=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For modelling, I have first trained the models on the complete training dataset (without considering only historical data). The reason for doing so was to get a clear picture of the accuracy of the models. Along with it, it was easier for performing hyperparameter tuning for different models. After selecting the best parameters for different models, I trained my models on the historical data only (as per the rules of kaggle competition). Then I tried to fine tune my hyperparameters using the historical data. However, as anticipated, it was computationally very expensive and it gave me memory error every time. So, I used the earlier best parameters only! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['casual', 'registered', 'count']:\n",
    "    mod_data['%s_log' % col] = np.log(mod_data[col] + 1) #transforming dependent variables with log \n",
    "    # 1 has been added to avoid infinity values at zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'season', 'holiday', 'workingday', 'weather', 'temp',\n",
       "       'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count',\n",
       "       'date', 'hour', 'month', 'weekday', 'year', 'peak_time',\n",
       "       'weekoff_count', 'best_condition', 'not_fav', 'sine_hr', 'cos_hr',\n",
       "       'casual_log', 'registered_log', 'count_log', 'day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_data.columns #checking columns of mod_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic functions to prepare data and predict models\n",
    "\n",
    "# error calculation function \n",
    "def get_rmsle(y_pred, y_actual):\n",
    "    diff = np.log(y_pred + 1) - np.log(y_actual + 1)\n",
    "    mean_error = np.square(diff).mean()\n",
    "    return np.sqrt(mean_error)\n",
    "\n",
    "# function for getting the mod_data dataframe\n",
    "def get_data():\n",
    "    data = mod_data[mod_data['count'] != 0].copy()\n",
    "    return data\n",
    "\n",
    "# splitting the data into train and validation set. # here we have used the cuttoff day as 15 instead of randomly splitting \n",
    "# the data. so, training data would be from day 1 to day 15, while the validation data would be day 16 onwards.\n",
    "def custom_train_test_split(data, cutoff_day=15):\n",
    "    train = data[data['day'] <= cutoff_day]\n",
    "    test = data[data['day'] > cutoff_day]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "# we would be predicting casual and registered values seperately, hence, preparing the labels for both.\n",
    "def prep_data(data, input_cols):\n",
    "    X = data[input_cols]\n",
    "    y_r = np.array(data['registered_log'])\n",
    "    y_c = np.array(data['casual_log'])\n",
    "    \n",
    "    return X, y_r, y_c\n",
    "\n",
    "\n",
    "# function to prepare and fit model and calculate error!\n",
    "def predict(input_cols, model_params={}):\n",
    "    \n",
    "    data = get_data()\n",
    "    train, test = custom_train_test_split(data)\n",
    "    X_train, y_train_r, y_train_c = prep_data(train, input_cols)\n",
    "    X_test, y_test_r, y_test_c = prep_data(test, input_cols)\n",
    "    model_params.update({\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 123,\n",
    "    })\n",
    "    model = RandomForestRegressor(**model_params)\n",
    "    model_r = model.fit(X_train, y_train_r)\n",
    "    y_pred_r = np.exp(model_r.predict(X_test)) - 1\n",
    "    model_c = model.fit(X_train, y_train_c)\n",
    "    y_pred_c = np.exp(model_c.predict(X_test)) - 1\n",
    "    y_pred_comb = np.round(y_pred_r + y_pred_c)\n",
    "    y_pred_comb[y_pred_comb < 0] = 0\n",
    "    y_test_comb = np.exp(y_test_r) + np.exp(y_test_c) - 2\n",
    "    score = get_rmsle(y_pred_comb, y_test_comb)\n",
    "    return (y_pred_comb, y_test_comb, score)\n",
    "\n",
    "# function to prepare and fit model and calculate error!\n",
    "def predict_on_validation_set(model, input_cols):\n",
    "    data = get_data()\n",
    "\n",
    "    train, test = custom_train_test_split(data)\n",
    "\n",
    "    X_train, y_train_r, y_train_c = prep_data(train, input_cols)\n",
    "    X_test, y_test_r, y_test_c = prep_data(test, input_cols)\n",
    "\n",
    "    model_r = model.fit(X_train, y_train_r)\n",
    "    y_pred_r = np.exp(model_r.predict(X_test)) - 1\n",
    "\n",
    "    model_c = model.fit(X_train, y_train_c)\n",
    "    y_pred_c = np.exp(model_c.predict(X_test)) - 1\n",
    "\n",
    "    y_pred_comb = np.round(y_pred_r + y_pred_c)\n",
    "    y_pred_comb[y_pred_comb < 0] = 0\n",
    "\n",
    "    y_test_comb = np.exp(y_test_r) + np.exp(y_test_c) - 2\n",
    "\n",
    "    score = get_rmsle(y_pred_comb, y_test_comb)\n",
    "    return (y_pred_comb, y_test_comb, score)\n",
    "\n",
    "df_test = mod_data[mod_data['count'] == 0].copy()\n",
    "\n",
    "# predict on test set & transform output back from log scale\n",
    "def predict_on_test_set(model, x_cols):\n",
    "    # prepare training set\n",
    "    df_train = mod_data[mod_data['count'] != 0].copy()\n",
    "    X_train = df_train[x_cols]\n",
    "    y_train_cas = np.array(df_train['casual_log'])\n",
    "    y_train_reg = np.array(df_train['registered_log'])\n",
    "\n",
    "    # prepare test set\n",
    "    X_test = df_test[x_cols]\n",
    "\n",
    "    casual_model = model.fit(X_train, y_train_cas)\n",
    "    y_pred_cas = casual_model.predict(X_test)\n",
    "    y_pred_cas = np.exp(y_pred_cas) - 1\n",
    "    registered_model = model.fit(X_train, y_train_reg)\n",
    "    y_pred_reg = registered_model.predict(X_test)\n",
    "    y_pred_reg = np.exp(y_pred_reg) - 1\n",
    "    # add casual & registered predictions together\n",
    "    return y_pred_cas + y_pred_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here I tried to perform the forward approach to determine best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: base_cols + []\n",
      "rmse: 0.46763734753166936\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: base_cols + ['month']\n",
      "rmse: 0.4585166613826167\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: base_cols + ['month', 'year']\n",
      "rmse: 0.3516819428964412\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: base_cols + ['month', 'year', 'peak_time']\n",
      "rmse: 0.3588887050658396\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: base_cols + ['month', 'year', 'peak_time', 'weekoff_count']\n",
      "rmse: 0.35514102551051635\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: base_cols + ['month', 'year', 'peak_time', 'weekoff_count', 'best_condition']\n",
      "rmse: 0.3567468954563939\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: base_cols + ['month', 'year', 'peak_time', 'weekoff_count', 'best_condition', 'not_fav']\n",
      "rmse: 0.35595848478952036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_cols = ['weather', 'temp', 'atemp', 'humidity', 'windspeed', 'holiday', 'workingday', 'season', 'sine_hr', 'cos_hr']\n",
    "\n",
    "try_cols = ['month', 'year', 'peak_time', 'weekoff_count', 'best_condition', 'not_fav']\n",
    "\n",
    "for i in range(0, len(try_cols) + 1):\n",
    "    new_cols = try_cols[:i]\n",
    "    all_cols = base_cols + new_cols\n",
    "    print('cols: base_cols + {}\\nrmse: {}\\n'.format(\n",
    "        new_cols, \n",
    "        predict(all_cols)\n",
    "    ))\n",
    "    \n",
    "# best_parameters selected = base_cols + ['month', 'year', 'peak_time', 'weekoff_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46657138733119374\n"
     ]
    }
   ],
   "source": [
    "#First, I tried to fit a randomforestRegressor with default parameters. Got rmsle score of 0.46657.\n",
    "params = {'n_estimators': 1000, 'max_depth': 15, 'random_state': 0, 'min_samples_split' : 5, 'n_jobs': -1} # random parameters\n",
    "rf_model = RandomForestRegressor(**params)\n",
    "rf_cols = [\n",
    "    'weather', 'temp', 'atemp', 'windspeed',\n",
    "    'workingday', 'season', 'holiday', 'not_fav',\n",
    "    'hour', 'peak_time'\n",
    "    ]\n",
    "\n",
    "(rf_p, rf_t, rf_score) = predict_on_validation_set(rf_model, rf_cols)\n",
    "print(rf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trees: 500, mss: 6, rmse: 0.3624808530696919\n",
      "trees: 500, mss: 8, rmse: 0.3607852450093484\n",
      "trees: 500, mss: 10, rmse: 0.35980365347162535\n",
      "trees: 500, mss: 12, rmse: 0.3588912529302141\n",
      "trees: 500, mss: 14, rmse: 0.3589586589712074\n",
      "trees: 1000, mss: 6, rmse: 0.36250783382492097\n",
      "trees: 1000, mss: 8, rmse: 0.3609776060495743\n",
      "trees: 1000, mss: 10, rmse: 0.3596985492605452\n",
      "trees: 1000, mss: 12, rmse: 0.3583742993569411\n",
      "trees: 1000, mss: 14, rmse: 0.35841353790238156\n",
      "trees: 1500, mss: 6, rmse: 0.36178199926268795\n",
      "trees: 1500, mss: 8, rmse: 0.36040295293113433\n",
      "trees: 1500, mss: 10, rmse: 0.3595663963215969\n",
      "trees: 1500, mss: 12, rmse: 0.35845279369155203\n",
      "trees: 1500, mss: 14, rmse: 0.3583228852913084\n",
      "best params: {'n_estimators': 1500, 'min_samples_split': 14, 'n_jobs': -1, 'random_state': 123}, rmse: 0.3583228852913084\n"
     ]
    }
   ],
   "source": [
    "x_cols = ['weather', 'temp', 'atemp', 'humidity', 'windspeed',\n",
    "          'holiday', 'workingday', 'season', 'hour', 'year', 'best_condition', 'not_fav']\n",
    "\n",
    "rf_model = RandomForestRegressor()\n",
    "# random forest param grid\n",
    "n_estimators = [500, 1000, 1500]\n",
    "min_samples_splits = [6, 8, 10, 12, 14]\n",
    "\n",
    "best_score, best_params = np.inf, None\n",
    "\n",
    "# loop through param grid & find top performer\n",
    "for ne in n_estimators:\n",
    "    for mss in min_samples_splits:\n",
    "        params = {'n_estimators': ne, 'min_samples_split': mss}\n",
    "        (rf_p, rf_t, score) = predict(x_cols, params)\n",
    "        print('trees: {}, mss: {}, rmse: {}'.format(ne, mss, score))\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_params = params\n",
    "            best_score = score\n",
    "            \n",
    "print('best params: {}, rmse: {}'.format(best_params, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3581244629806633\n"
     ]
    }
   ],
   "source": [
    "# training with the best parameters. Hyperparameter tuning paid! the error has reduced significantly!\n",
    "\n",
    "params = {'n_estimators': 1500, 'max_depth': 15, 'random_state': 123, 'min_samples_split' : 14, 'n_jobs': -1}\n",
    "rf_model = RandomForestRegressor(**params)\n",
    "rf_cols = ['weather', 'temp', 'atemp', 'humidity', 'windspeed',\n",
    "           'holiday', 'workingday', 'season', 'hour', 'year',\n",
    "           'best_condition', 'not_fav']\n",
    "\n",
    "(rf_p, rf_t, rf_score) = predict_on_validation_set(rf_model, rf_cols)\n",
    "print(rf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Gradient Boosting Regressor with default parameters. Combining with the output of Random Forest Regressor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3311940527956884\n"
     ]
    }
   ],
   "source": [
    "# Training with default parameters!\n",
    "params = {'n_estimators': 150, 'max_depth': 5, 'random_state': 0, 'min_samples_leaf' : 10, 'learning_rate': 0.1, 'subsample': 0.7, 'loss': 'ls'}\n",
    "gbm_model = GradientBoostingRegressor(**params)\n",
    "gbm_cols = [\n",
    "    'weather', 'temp', 'atemp', 'humidity', 'windspeed',\n",
    "    'holiday', 'workingday', 'season',\n",
    "    'hour', 'year', 'best_condition']\n",
    "\n",
    "(gbm_p, gbm_t, gbm_score) = predict_on_validation_set(gbm_model, gbm_cols)\n",
    "print(gbm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By combining the predictions of random forest and gradient boosting (20-80%), \n",
    "## I got Kaggle Score of 0.38024!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### However, as mentioned earlier, I have used the complete training dataset for training the model. :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred = predict_on_test_set(rf_model, rf_cols)\n",
    "gbm_pred = predict_on_test_set(gbm_model, gbm_cols)\n",
    "y_pred = np.round(.2*rf_pred + .8*gbm_pred)\n",
    "# output predictions for submission\n",
    "df_test['count'] = y_pred\n",
    "final_df = df_test[['datetime', 'count']].copy()\n",
    "final_df.to_csv('submit4.csv', index=False) # kaggle score 0.38024!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how does the GradientBoostingRegressor do with default parameters on kaggle (without combining it with randomforest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3339104996093259\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': 150, 'max_depth': 5, 'random_state': 0, 'min_samples_leaf' : 10, 'learning_rate': 0.1, 'subsample': 0.7, 'loss': 'ls'}\n",
    "gbm_model = GradientBoostingRegressor(**params)\n",
    "gbm_cols = [\n",
    "    'weather', 'temp', 'atemp', 'humidity', 'windspeed',\n",
    "    'holiday', 'workingday', 'season',\n",
    "    'hour', 'year', 'best_condition', 'not_fav']\n",
    "\n",
    "(gbm_p, gbm_t, gbm_score) = predict(gbm_model, gbm_cols)\n",
    "print(gbm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I got a score of 0.38889 on Kaggle with default parameters of GBM.\n",
    "#### This implies that we have done good feature engineering. Let's fine tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_pred = predict_on_test_set(rf_model, rf_cols)\n",
    "gbm_pred = predict_on_test_set(gbm_model, gbm_cols)\n",
    "y_pred = np.round(gbm_pred)\n",
    "# output predictions for submission\n",
    "df_test['count'] = y_pred\n",
    "final_df = df_test[['datetime', 'count']].copy()\n",
    "final_df.to_csv('submit5.csv', index=False) # kaggle score 0.38889"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning for GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gbm(input_cols, model_params={}):\n",
    "    \n",
    "    data = get_data()\n",
    "    \n",
    "    train, test = custom_train_test_split(data)\n",
    "    \n",
    "    X_train, y_train_r, y_train_c = prep_data(train, input_cols)\n",
    "    X_test, y_test_r, y_test_c = prep_data(test, input_cols)\n",
    "\n",
    "    model_params.update({'random_state': 123})\n",
    "    model = GradientBoostingRegressor(**model_params)\n",
    "    \n",
    "    model_r = model.fit(X_train, y_train_r)\n",
    "    y_pred_r = np.exp(model_r.predict(X_test)) - 1\n",
    "    \n",
    "    model_c = model.fit(X_train, y_train_c)\n",
    "    y_pred_c = np.exp(model_c.predict(X_test)) - 1\n",
    "\n",
    "    y_pred_comb = np.round(y_pred_r + y_pred_c)\n",
    "    y_pred_comb[y_pred_comb < 0] = 0\n",
    "    \n",
    "    y_test_comb = np.exp(y_test_r) + np.exp(y_test_c) - 2\n",
    "\n",
    "    score = get_rmsle(y_pred_comb, y_test_comb)\n",
    "    return (y_pred_comb, y_test_comb, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trees: 150, mss: 6, rmse: 0.35330101166320665\n",
      "trees: 150, mss: 6, rmse: 0.3470252420547264\n",
      "trees: 150, mss: 6, rmse: 0.34556942136931673\n",
      "trees: 150, mss: 6, rmse: 0.3486512911656853\n",
      "trees: 150, mss: 6, rmse: 0.6936687676390332\n",
      "trees: 150, mss: 6, rmse: 0.6931471005362088\n",
      "trees: 150, mss: 6, rmse: 0.694288700457273\n",
      "trees: 150, mss: 6, rmse: 0.693244272421617\n",
      "trees: 150, mss: 8, rmse: 0.34574854766037133\n",
      "trees: 150, mss: 8, rmse: 0.34748139354025953\n",
      "trees: 150, mss: 8, rmse: 0.3450228583608803\n",
      "trees: 150, mss: 8, rmse: 0.3493410563176933\n",
      "trees: 150, mss: 8, rmse: 0.6937250867338061\n",
      "trees: 150, mss: 8, rmse: 0.6931471005362088\n",
      "trees: 150, mss: 8, rmse: 0.694288700457273\n",
      "trees: 150, mss: 8, rmse: 0.693244272421617\n",
      "trees: 150, mss: 10, rmse: 0.3498323250528359\n",
      "trees: 150, mss: 10, rmse: 0.3437897406420964\n",
      "trees: 150, mss: 10, rmse: 0.34581040933420765\n",
      "trees: 150, mss: 10, rmse: 0.350775381671304\n",
      "trees: 150, mss: 10, rmse: 0.6937250867338061\n",
      "trees: 150, mss: 10, rmse: 0.6931471005362088\n",
      "trees: 150, mss: 10, rmse: 0.694288700457273\n",
      "trees: 150, mss: 10, rmse: 0.693244272421617\n",
      "trees: 150, mss: 12, rmse: 0.3513736118484886\n",
      "trees: 150, mss: 12, rmse: 0.34652751105736285\n",
      "trees: 150, mss: 12, rmse: 0.3444844132112767\n",
      "trees: 150, mss: 12, rmse: 0.34633450327631904\n",
      "trees: 150, mss: 12, rmse: 0.6936708125690768\n",
      "trees: 150, mss: 12, rmse: 0.6931471005362088\n",
      "trees: 150, mss: 12, rmse: 0.694288700457273\n",
      "trees: 150, mss: 12, rmse: 0.693244272421617\n",
      "trees: 150, mss: 14, rmse: 0.35167478751209175\n",
      "trees: 150, mss: 14, rmse: 0.3489140814930019\n",
      "trees: 150, mss: 14, rmse: 0.34460999825899874\n",
      "trees: 150, mss: 14, rmse: 0.34899830027215756\n",
      "trees: 150, mss: 14, rmse: 0.6936708125690768\n",
      "trees: 150, mss: 14, rmse: 0.6931471005362088\n",
      "trees: 150, mss: 14, rmse: 0.694288700457273\n",
      "trees: 150, mss: 14, rmse: 0.693244272421617\n",
      "trees: 200, mss: 6, rmse: 0.34607960326170906\n",
      "trees: 200, mss: 6, rmse: 0.33928230528342973\n",
      "trees: 200, mss: 6, rmse: 0.34220062949984925\n",
      "trees: 200, mss: 6, rmse: 0.34084446781977734\n",
      "trees: 200, mss: 6, rmse: 0.618320555355608\n",
      "trees: 200, mss: 6, rmse: 0.6183997693188348\n",
      "trees: 200, mss: 6, rmse: 0.6205888333588891\n",
      "trees: 200, mss: 6, rmse: 0.6187166151842551\n",
      "trees: 200, mss: 8, rmse: 0.3405018898738843\n",
      "trees: 200, mss: 8, rmse: 0.33798196051178947\n",
      "trees: 200, mss: 8, rmse: 0.3397708714488841\n",
      "trees: 200, mss: 8, rmse: 0.3374788984686384\n",
      "trees: 200, mss: 8, rmse: 0.6180590314387501\n",
      "trees: 200, mss: 8, rmse: 0.6183997693188348\n",
      "trees: 200, mss: 8, rmse: 0.6205888333588891\n",
      "trees: 200, mss: 8, rmse: 0.6187166151842551\n",
      "trees: 200, mss: 10, rmse: 0.34242768077740976\n",
      "trees: 200, mss: 10, rmse: 0.3381917920641077\n",
      "trees: 200, mss: 10, rmse: 0.3393410516970734\n",
      "trees: 200, mss: 10, rmse: 0.34101019700704144\n",
      "trees: 200, mss: 10, rmse: 0.6180701870207445\n",
      "trees: 200, mss: 10, rmse: 0.6183997693188348\n",
      "trees: 200, mss: 10, rmse: 0.6205888333588891\n",
      "trees: 200, mss: 10, rmse: 0.6187166151842551\n",
      "trees: 200, mss: 12, rmse: 0.34661367034090373\n",
      "trees: 200, mss: 12, rmse: 0.339852887775477\n",
      "trees: 200, mss: 12, rmse: 0.3411899276379797\n",
      "trees: 200, mss: 12, rmse: 0.3395963254335173\n",
      "trees: 200, mss: 12, rmse: 0.6186319729187625\n",
      "trees: 200, mss: 12, rmse: 0.6184229560183852\n",
      "trees: 200, mss: 12, rmse: 0.6205888333588891\n",
      "trees: 200, mss: 12, rmse: 0.6187166151842551\n",
      "trees: 200, mss: 14, rmse: 0.34580040825562397\n",
      "trees: 200, mss: 14, rmse: 0.34070129629807777\n",
      "trees: 200, mss: 14, rmse: 0.34077001016120256\n",
      "trees: 200, mss: 14, rmse: 0.340994662084611\n",
      "trees: 200, mss: 14, rmse: 0.6186298821320256\n",
      "trees: 200, mss: 14, rmse: 0.6184331176591861\n",
      "trees: 200, mss: 14, rmse: 0.6205986393629859\n",
      "trees: 200, mss: 14, rmse: 0.6187166151842551\n",
      "trees: 500, mss: 6, rmse: 0.34034043036749795\n",
      "trees: 500, mss: 6, rmse: 0.33536569401305083\n",
      "trees: 500, mss: 6, rmse: 0.3351193442966787\n",
      "trees: 500, mss: 6, rmse: 0.33802826089413823\n",
      "trees: 500, mss: 6, rmse: 0.46063782327023756\n",
      "trees: 500, mss: 6, rmse: 0.4662143177251013\n",
      "trees: 500, mss: 6, rmse: 0.46043040859612216\n",
      "trees: 500, mss: 6, rmse: 0.46114491311107186\n",
      "trees: 500, mss: 8, rmse: 0.33704823684467705\n",
      "trees: 500, mss: 8, rmse: 0.3360462376775438\n",
      "trees: 500, mss: 8, rmse: 0.3325215678106417\n",
      "trees: 500, mss: 8, rmse: 0.33639211857025514\n",
      "trees: 500, mss: 8, rmse: 0.46067291810515687\n",
      "trees: 500, mss: 8, rmse: 0.4646733865436856\n",
      "trees: 500, mss: 8, rmse: 0.45980301198021756\n",
      "trees: 500, mss: 8, rmse: 0.46126890994363684\n",
      "trees: 500, mss: 10, rmse: 0.339467955550889\n",
      "trees: 500, mss: 10, rmse: 0.33354892535111347\n",
      "trees: 500, mss: 10, rmse: 0.33465285333296796\n",
      "trees: 500, mss: 10, rmse: 0.3350183209929609\n",
      "trees: 500, mss: 10, rmse: 0.4602256448873823\n",
      "trees: 500, mss: 10, rmse: 0.46458371193694803\n",
      "trees: 500, mss: 10, rmse: 0.46028614619834785\n",
      "trees: 500, mss: 10, rmse: 0.46127927280412107\n",
      "trees: 500, mss: 12, rmse: 0.33935434556588473\n",
      "trees: 500, mss: 12, rmse: 0.33341852446489884\n",
      "trees: 500, mss: 12, rmse: 0.3336044203760676\n",
      "trees: 500, mss: 12, rmse: 0.3330943796698533\n",
      "trees: 500, mss: 12, rmse: 0.45885607469898587\n",
      "trees: 500, mss: 12, rmse: 0.46455001402869\n",
      "trees: 500, mss: 12, rmse: 0.4602872379388233\n",
      "trees: 500, mss: 12, rmse: 0.46147130150844107\n",
      "trees: 500, mss: 14, rmse: 0.3383907137509256\n",
      "trees: 500, mss: 14, rmse: 0.33526451815281266\n",
      "trees: 500, mss: 14, rmse: 0.33576356465592905\n",
      "trees: 500, mss: 14, rmse: 0.33401605139065466\n",
      "trees: 500, mss: 14, rmse: 0.4596627851479365\n",
      "trees: 500, mss: 14, rmse: 0.4666384920903054\n",
      "trees: 500, mss: 14, rmse: 0.46070122138608555\n",
      "trees: 500, mss: 14, rmse: 0.4616755864681868\n",
      "trees: 1000, mss: 6, rmse: 0.34580881374393535\n",
      "trees: 1000, mss: 6, rmse: 0.33928126771236516\n",
      "trees: 1000, mss: 6, rmse: 0.3424344358648676\n",
      "trees: 1000, mss: 6, rmse: 0.3413986914271471\n",
      "trees: 1000, mss: 6, rmse: 0.3755621033876846\n",
      "trees: 1000, mss: 6, rmse: 0.3748445448865949\n",
      "trees: 1000, mss: 6, rmse: 0.37635908563454257\n",
      "trees: 1000, mss: 6, rmse: 0.3774983231043974\n",
      "trees: 1000, mss: 8, rmse: 0.34175775755327653\n",
      "trees: 1000, mss: 8, rmse: 0.34158478222780975\n",
      "trees: 1000, mss: 8, rmse: 0.33981969482471663\n",
      "trees: 1000, mss: 8, rmse: 0.34222082433665557\n",
      "trees: 1000, mss: 8, rmse: 0.37382118889476856\n",
      "trees: 1000, mss: 8, rmse: 0.37531596426765\n",
      "trees: 1000, mss: 8, rmse: 0.3753649735722137\n",
      "trees: 1000, mss: 8, rmse: 0.3775881436237168\n",
      "trees: 1000, mss: 10, rmse: 0.34291091324473116\n",
      "trees: 1000, mss: 10, rmse: 0.33774081098842246\n",
      "trees: 1000, mss: 10, rmse: 0.341445002353412\n",
      "trees: 1000, mss: 10, rmse: 0.33949893705129724\n",
      "trees: 1000, mss: 10, rmse: 0.374945700790687\n",
      "trees: 1000, mss: 10, rmse: 0.3756676800923948\n",
      "trees: 1000, mss: 10, rmse: 0.37620725443007375\n",
      "trees: 1000, mss: 10, rmse: 0.3770537628690362\n",
      "trees: 1000, mss: 12, rmse: 0.34400340212554126\n",
      "trees: 1000, mss: 12, rmse: 0.33844253605405533\n",
      "trees: 1000, mss: 12, rmse: 0.34060903459313707\n",
      "trees: 1000, mss: 12, rmse: 0.33897370999568777\n",
      "trees: 1000, mss: 12, rmse: 0.37489044373296715\n",
      "trees: 1000, mss: 12, rmse: 0.3756037227989394\n",
      "trees: 1000, mss: 12, rmse: 0.37573594410557515\n",
      "trees: 1000, mss: 12, rmse: 0.3775832412310781\n",
      "trees: 1000, mss: 14, rmse: 0.3420948335035532\n",
      "trees: 1000, mss: 14, rmse: 0.3396709806050615\n",
      "trees: 1000, mss: 14, rmse: 0.3417119458911385\n",
      "trees: 1000, mss: 14, rmse: 0.3406884540721342\n",
      "trees: 1000, mss: 14, rmse: 0.37605967539570523\n",
      "trees: 1000, mss: 14, rmse: 0.3756305829090957\n",
      "trees: 1000, mss: 14, rmse: 0.3761843080799902\n",
      "trees: 1000, mss: 14, rmse: 0.37740722930488313\n",
      "trees: 1500, mss: 6, rmse: 0.35154804748798385\n",
      "trees: 1500, mss: 6, rmse: 0.3445193810809706\n",
      "trees: 1500, mss: 6, rmse: 0.3455915578792387\n",
      "trees: 1500, mss: 6, rmse: 0.34839799229444657\n",
      "trees: 1500, mss: 6, rmse: 0.34754771431804443\n",
      "trees: 1500, mss: 6, rmse: 0.34715653119592543\n",
      "trees: 1500, mss: 6, rmse: 0.345855545855122\n",
      "trees: 1500, mss: 6, rmse: 0.34752005173659095\n",
      "trees: 1500, mss: 8, rmse: 0.3487417737951334\n",
      "trees: 1500, mss: 8, rmse: 0.34322101467579186\n",
      "trees: 1500, mss: 8, rmse: 0.34370998491618054\n",
      "trees: 1500, mss: 8, rmse: 0.3485219497703596\n",
      "trees: 1500, mss: 8, rmse: 0.3475643429475424\n",
      "trees: 1500, mss: 8, rmse: 0.34751654406027105\n",
      "trees: 1500, mss: 8, rmse: 0.34625822374572096\n",
      "trees: 1500, mss: 8, rmse: 0.34798937685254616\n",
      "trees: 1500, mss: 10, rmse: 0.34741496748070444\n",
      "trees: 1500, mss: 10, rmse: 0.34303769989647853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trees: 1500, mss: 10, rmse: 0.3442548578602096\n",
      "trees: 1500, mss: 10, rmse: 0.34495127377298407\n",
      "trees: 1500, mss: 10, rmse: 0.3478434168821772\n",
      "trees: 1500, mss: 10, rmse: 0.3471131965649173\n",
      "trees: 1500, mss: 10, rmse: 0.3463264558382087\n",
      "trees: 1500, mss: 10, rmse: 0.34738653712080186\n",
      "trees: 1500, mss: 12, rmse: 0.34665215489970364\n",
      "trees: 1500, mss: 12, rmse: 0.34205207584521585\n",
      "trees: 1500, mss: 12, rmse: 0.3436729617025161\n",
      "trees: 1500, mss: 12, rmse: 0.34403579871232454\n",
      "trees: 1500, mss: 12, rmse: 0.34799255967848547\n",
      "trees: 1500, mss: 12, rmse: 0.3480944532754327\n",
      "trees: 1500, mss: 12, rmse: 0.34634510325102336\n",
      "trees: 1500, mss: 12, rmse: 0.34754606773186214\n",
      "trees: 1500, mss: 14, rmse: 0.34731198485956055\n",
      "trees: 1500, mss: 14, rmse: 0.3449036007212849\n",
      "trees: 1500, mss: 14, rmse: 0.34389089995555394\n",
      "trees: 1500, mss: 14, rmse: 0.34708107010117784\n",
      "trees: 1500, mss: 14, rmse: 0.35021031162976257\n",
      "trees: 1500, mss: 14, rmse: 0.3491088481384823\n",
      "trees: 1500, mss: 14, rmse: 0.3465882015324493\n",
      "trees: 1500, mss: 14, rmse: 0.346417347402829\n",
      "trees: 1800, mss: 6, rmse: 0.35535195244474954\n",
      "trees: 1800, mss: 6, rmse: 0.34850000612583587\n",
      "trees: 1800, mss: 6, rmse: 0.3506715209445259\n",
      "trees: 1800, mss: 6, rmse: 0.35257062014860563\n",
      "trees: 1800, mss: 6, rmse: 0.34320957761345056\n",
      "trees: 1800, mss: 6, rmse: 0.34244177085313315\n",
      "trees: 1800, mss: 6, rmse: 0.33939451312575053\n",
      "trees: 1800, mss: 6, rmse: 0.34159693527246865\n",
      "trees: 1800, mss: 8, rmse: 0.3516540449401549\n",
      "trees: 1800, mss: 8, rmse: 0.34572594283446806\n",
      "trees: 1800, mss: 8, rmse: 0.3462143855417833\n",
      "trees: 1800, mss: 8, rmse: 0.35093010288414017\n",
      "trees: 1800, mss: 8, rmse: 0.34302510510938267\n",
      "trees: 1800, mss: 8, rmse: 0.34278635923589007\n",
      "trees: 1800, mss: 8, rmse: 0.3406185948226774\n",
      "trees: 1800, mss: 8, rmse: 0.3412133629345054\n",
      "trees: 1800, mss: 10, rmse: 0.3514705571356225\n",
      "trees: 1800, mss: 10, rmse: 0.3440964360694417\n",
      "trees: 1800, mss: 10, rmse: 0.3477673983122424\n",
      "trees: 1800, mss: 10, rmse: 0.349227544075041\n",
      "trees: 1800, mss: 10, rmse: 0.3431025435879413\n",
      "trees: 1800, mss: 10, rmse: 0.342295687446504\n",
      "trees: 1800, mss: 10, rmse: 0.340037777871362\n",
      "trees: 1800, mss: 10, rmse: 0.34074651178546544\n",
      "trees: 1800, mss: 12, rmse: 0.34954346201315856\n",
      "trees: 1800, mss: 12, rmse: 0.34440996560553383\n",
      "trees: 1800, mss: 12, rmse: 0.34690128667297354\n",
      "trees: 1800, mss: 12, rmse: 0.34695337481045124\n",
      "trees: 1800, mss: 12, rmse: 0.3434384391267382\n",
      "trees: 1800, mss: 12, rmse: 0.3424148191712686\n",
      "trees: 1800, mss: 12, rmse: 0.34115168094630083\n",
      "trees: 1800, mss: 12, rmse: 0.3414893713189529\n",
      "trees: 1800, mss: 14, rmse: 0.3501027473812902\n",
      "trees: 1800, mss: 14, rmse: 0.34742124124962803\n",
      "trees: 1800, mss: 14, rmse: 0.34830736709106574\n",
      "trees: 1800, mss: 14, rmse: 0.350820917239991\n",
      "trees: 1800, mss: 14, rmse: 0.3447712450776636\n",
      "trees: 1800, mss: 14, rmse: 0.34335951777710666\n",
      "trees: 1800, mss: 14, rmse: 0.3411479204866562\n",
      "trees: 1800, mss: 14, rmse: 0.3411630114863877\n",
      "best params: {'n_estimators': 500, 'min_samples_leaf': 8, 'learning_rate': 0.1, 'subsample': 0.7, 'random_state': 123}, rmse: 0.3325215678106417\n"
     ]
    }
   ],
   "source": [
    "#Performing hyperparameter tuning of gradient boosting regression. \n",
    "# best parameters came out to be: best params: \n",
    "# {'n_estimators': 500, 'min_samples_leaf': 8, 'learning_rate': 0.1, 'subsample': 0.7, 'random_state': 123}\n",
    "\n",
    "x_cols = [\n",
    "    'weather', 'temp', 'atemp', 'humidity', 'windspeed',\n",
    "    'holiday', 'workingday', 'season',\n",
    "    'hour', 'year', 'best_condition', 'not_fav']\n",
    "\n",
    "rf_model = GradientBoostingRegressor()\n",
    "# gradient boosting param grid\n",
    "n_estimators = [150, 200, 500, 1000, 1500, 1800]\n",
    "min_samples_leaf = [6, 8, 10, 12, 14]\n",
    "learning_rate = [0.1, 0.01]\n",
    "subsample = [0.5, 0.6, 0.7, 0.8]\n",
    "\n",
    "best_score, best_params = np.inf, None\n",
    "\n",
    "# loop through param grid & find top performer\n",
    "for ne in n_estimators:\n",
    "    for mss in min_samples_leaf:\n",
    "        for lr in learning_rate:\n",
    "            for sub in subsample:\n",
    "                params = {'n_estimators': ne, 'min_samples_leaf': mss, 'learning_rate': lr, 'subsample': sub}\n",
    "                (rf_p, rf_t, score) = predict_gbm(x_cols, params)\n",
    "                print('trees: {}, mss: {}, rmse: {}'.format(ne, mss, score))\n",
    "\n",
    "                if score < best_score:\n",
    "                    best_params = params\n",
    "                    best_score = score\n",
    "            \n",
    "print('best params: {}, rmse: {}'.format(best_params, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions for training the model on each months (still not using historic data, just using each month for\n",
    "# both years as training data). By this, I would be able to understand if my algorithm is accurately predicting the count - \n",
    "# for each month!\n",
    "\n",
    "def custom_train_test_split1(data, month, cutoff_day=15):\n",
    "    train = data[(data['day'] <= cutoff_day) & (data['month'] == month)]\n",
    "    test = data[(data['day'] > cutoff_day) & (data['month'] == month)]\n",
    "    \n",
    "    return train, test\n",
    "    \n",
    "def predict_1(model, input_cols):\n",
    "    data = get_data()\n",
    "    \n",
    "    for month in range(1, 13):\n",
    "    \n",
    "        train, test = custom_train_test_split1(data, month)\n",
    "\n",
    "        X_train, y_train_r, y_train_c = prep_data(train, input_cols)\n",
    "        X_test, y_test_r, y_test_c = prep_data(test, input_cols)\n",
    "\n",
    "        model_r = model.fit(X_train, y_train_r)\n",
    "        y_pred_r = np.exp(model_r.predict(X_test)) - 1\n",
    "\n",
    "        model_c = model.fit(X_train, y_train_c)\n",
    "        y_pred_c = np.exp(model_c.predict(X_test)) - 1\n",
    "\n",
    "        y_pred_comb = np.round(y_pred_r + y_pred_c)\n",
    "        y_pred_comb[y_pred_comb < 0] = 0\n",
    "\n",
    "        y_test_comb = np.exp(y_test_r) + np.exp(y_test_c) - 2\n",
    "\n",
    "        score = get_rmsle(y_pred_comb, y_test_comb)\n",
    "        print(\"month {}, score {}\".format(month, score))\n",
    "    return (y_pred_comb, y_test_comb, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month 1, score 0.5156243744826036\n",
      "month 2, score 0.41161759063326836\n",
      "month 3, score 0.40848867269659617\n",
      "month 4, score 0.573548815741194\n",
      "month 5, score 0.3608484579611672\n",
      "month 6, score 0.3166987362456103\n",
      "month 7, score 0.283281206225008\n",
      "month 8, score 0.3481298011184439\n",
      "month 9, score 0.4254745493446438\n",
      "month 10, score 0.39954383474426486\n",
      "month 11, score 0.3712948590399787\n",
      "month 12, score 0.393385934982505\n",
      "0.393385934982505\n"
     ]
    }
   ],
   "source": [
    "# Based on the output, it can be seen that the algorithm's rmsle score for month 1-5 is low; however, it gets improved after that.\n",
    "\n",
    "best_params = {'n_estimators': 500, 'min_samples_leaf': 8, 'learning_rate': 0.1, 'subsample': 0.7, 'random_state': 123}\n",
    "\n",
    "gbm_model = GradientBoostingRegressor(**best_params)\n",
    "gbm_cols = [\n",
    "    'weather', 'temp', 'atemp', 'humidity', 'windspeed',\n",
    "    'holiday', 'workingday', 'season',\n",
    "    'hour', 'year', 'best_condition', 'not_fav']\n",
    "\n",
    "(gbm_p, gbm_t, gbm_score) = predict_1(gbm_model, gbm_cols)\n",
    "print(gbm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Code: Defining function to predict on historical data (as per kaggle rule) i.e., only using information which was available prior to the time of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test_set_1(model, x_cols):\n",
    "    # prepare training set\n",
    "    df_ = mod_data[mod_data['count'] != 0].copy()\n",
    "    df_t = mod_data[mod_data['count'] == 0].copy()\n",
    "    years = [2011, 2012]\n",
    "    df_batch = pd.DataFrame()\n",
    "    df_result = pd.DataFrame()\n",
    "    result = np.empty((0,1), float)\n",
    "    y_result = []\n",
    "    for yr in years:\n",
    "        for i in range(1, 13):\n",
    "            df_train = df_[(df_['month'] == i) & (df_['year'] == yr)]\n",
    "            df_batch = df_batch.append(df_train)\n",
    "            \n",
    "            X_train = df_batch[x_cols]\n",
    "            y_train_cas = np.array(df_batch['casual_log'])\n",
    "            y_train_reg = np.array(df_batch['registered_log'])\n",
    "\n",
    "            # prepare test set\n",
    "            df_test = df_t[(df_t['month'] == i) & (df_t['year'] == yr)]\n",
    "            X_test = df_test[x_cols]\n",
    "\n",
    "            casual_model = model.fit(X_train, y_train_cas)\n",
    "            y_pred_cas = casual_model.predict(X_test)\n",
    "            y_pred_cas = np.exp(y_pred_cas) - 1\n",
    "            registered_model = model.fit(X_train, y_train_reg)\n",
    "            y_pred_reg = registered_model.predict(X_test)\n",
    "            y_pred_reg = np.exp(y_pred_reg) - 1\n",
    "            y_count = y_pred_cas + y_pred_reg\n",
    "            print(y_count.size)\n",
    "            y_result = np.concatenate([y_result, y_count])\n",
    "    return y_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on the test dataset using Gradient Boosting Regressor!\n",
    "## I got Kaggle Score of 0.42099!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n",
      "203\n",
      "284\n",
      "264\n",
      "288\n",
      "264\n",
      "288\n",
      "275\n",
      "264\n",
      "288\n",
      "263\n",
      "285\n",
      "288\n",
      "237\n",
      "288\n",
      "264\n",
      "288\n",
      "264\n",
      "288\n",
      "288\n",
      "264\n",
      "252\n",
      "263\n",
      "286\n"
     ]
    }
   ],
   "source": [
    "best_params = {'n_estimators': 500, 'min_samples_leaf': 8, 'learning_rate': 0.1, 'subsample': 0.7, 'random_state': 123}\n",
    "\n",
    "gbm_model = GradientBoostingRegressor(**best_params)\n",
    "gbm_cols = [\n",
    "    'weather', 'temp', 'atemp', 'humidity', 'windspeed',\n",
    "    'holiday', 'workingday', 'season',\n",
    "    'hour', 'year', 'best_condition', 'not_fav']\n",
    "\n",
    "gbm_pred = predict_on_test_set_1(gbm_model, gbm_cols)\n",
    "\n",
    "y_pred = np.round(gbm_pred)\n",
    "\n",
    "# output predictions for submission\n",
    "\n",
    "df_test['count'] = y_pred\n",
    "final_df = df_test[['datetime', 'count']].copy()\n",
    "final_df.to_csv('submit6_1.csv', index=False) # kaggle score 0.42099 (best params)\n",
    "\n",
    "# Please ignore the outputs below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on the test dataset using Gradient Boosting Regressor; but dropping one last column 'not_fav'! By doing so, it improved the score. \n",
    "## I got Kaggle Score of 0.41872!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n",
      "203\n",
      "284\n",
      "264\n",
      "288\n",
      "264\n",
      "288\n",
      "275\n",
      "264\n",
      "288\n",
      "263\n",
      "285\n",
      "288\n",
      "237\n",
      "288\n",
      "264\n",
      "288\n",
      "264\n",
      "288\n",
      "288\n",
      "264\n",
      "252\n",
      "263\n",
      "286\n"
     ]
    }
   ],
   "source": [
    "gbm_cols = [\n",
    "    'weather', 'temp', 'atemp', 'humidity', 'windspeed',\n",
    "    'holiday', 'workingday', 'season',\n",
    "    'hour', 'year', 'best_condition']\n",
    "best_params = {'n_estimators': 500, 'min_samples_leaf': 8, 'learning_rate': 0.1, 'subsample': 0.7, 'random_state': 123}\n",
    "gbm_model = GradientBoostingRegressor(**best_params)\n",
    "\n",
    "gbm_pred = predict_on_test_set_1(gbm_model, gbm_cols)\n",
    "\n",
    "y_pred = np.round(gbm_pred)\n",
    "\n",
    "# output predictions for submission\n",
    "\n",
    "df_test['count'] = y_pred\n",
    "final_df = df_test[['datetime', 'count']].copy()\n",
    "final_df.to_csv('submit7_1.csv', index=False) # kaggle score 0.41872 (best_params)\n",
    "\n",
    "# Please ignore the outputs below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n",
      "203\n",
      "284\n",
      "264\n",
      "288\n",
      "264\n",
      "288\n",
      "275\n",
      "264\n",
      "288\n",
      "263\n",
      "285\n",
      "288\n",
      "237\n",
      "288\n",
      "264\n",
      "288\n",
      "264\n",
      "288\n",
      "288\n",
      "264\n",
      "252\n",
      "263\n",
      "286\n"
     ]
    }
   ],
   "source": [
    "#Training RandomForest with best_parameters and best_columns. RandomForest does not perform as good as GradientBoosting but \\n\n",
    "# by combining there outputs I got better score at Kaggle! (show in next cell)\n",
    "\n",
    "params = {'n_estimators': 1500, 'max_depth': 15, 'random_state': 123, 'min_samples_split' : 14, 'n_jobs': -1}\n",
    "rf_model = RandomForestRegressor(**params)\n",
    "rf_cols = [\n",
    "    'weather', 'temp', 'atemp', 'windspeed',\n",
    "    'workingday', 'season', 'holiday', 'not_fav',\n",
    "    'hour', 'peak_time'\n",
    "    ]\n",
    "\n",
    "rnd_pred = predict_on_test_set_1(rf_model, rf_cols)\n",
    "\n",
    "rf_y_pred = np.round(rnd_pred)\n",
    "\n",
    "# output predictions for submission\n",
    "\n",
    "df_test['count'] = rf_y_pred\n",
    "final_df = df_test[['datetime', 'count']].copy()\n",
    "final_df.to_csv('submit9.csv', index=False) # kaggle score 0.48258\n",
    "\n",
    "# Please ignore the outputs below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Predictions of GradientBoosting and RandomForest Improves the overall Kaggle Score. I got Kaggle Score of 0.41490 (Best)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Randomforest and GradientBoosting predictions by 20-80%. We got the score of 0.41490\n",
    "y_pred = np.round(.2*rnd_pred + .8*gbm_pred)\n",
    "# output predictions for submission\n",
    "df_test['count'] = y_pred\n",
    "final_df = df_test[['datetime', 'count']].copy()\n",
    "final_df.to_csv('submit10.csv', index=False) # kaggle score (BEST) 0.41490"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n",
      "203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286\n"
     ]
    }
   ],
   "source": [
    "# Predicting with default parameters of SVR\n",
    "svr_cols = ['weather', 'temp', 'atemp', 'humidity', 'windspeed', 'month', 'year', 'peak_time',\n",
    "            'weekoff_count', 'best_condition', 'not_fav', 'season_is_1',\n",
    "            'season_is_2', 'season_is_3', 'season_is_4', 'holiday_is_0',\n",
    "            'holiday_is_1', 'workingday_is_0', 'workingday_is_1', 'hour_is_0',\n",
    "            'hour_is_1', 'hour_is_2', 'hour_is_3', 'hour_is_4', 'hour_is_5',\n",
    "            'hour_is_6', 'hour_is_7', 'hour_is_8', 'hour_is_9', 'hour_is_10',\n",
    "            'hour_is_11', 'hour_is_12', 'hour_is_13', 'hour_is_14', 'hour_is_15',\n",
    "            'hour_is_16', 'hour_is_17', 'hour_is_18', 'hour_is_19', 'hour_is_20',\n",
    "            'hour_is_21', 'hour_is_22', 'hour_is_23']\n",
    "\n",
    "clf_svr = SVR(C=1.0, epsilon=0.2)\n",
    "\n",
    "df_test_svr = pd.read_csv(r\"D:\\OneDrive - Queen's University\\ECE\\Statistical Learning\\Midterm\\test.csv\")\n",
    "mod_svr = predict_on_test_set_2(one_data, clf_svr, svr_cols)\n",
    "\n",
    "\n",
    "svr_y_pred = np.round(mod_svr)\n",
    "\n",
    "# output predictions for submission\n",
    "\n",
    "df_test_svr['count'] = svr_y_pred\n",
    "final_df = df_test_svr[['datetime', 'count']].copy()\n",
    "final_df.to_csv('submit15.csv', index=False) # kaggle score 1.10676 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:266: UserWarning: The total space of parameters 5 is smaller than n_iter=10. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:  1.5min remaining:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]{'kernel': 'rbf', 'gamma': 'auto', 'epsilon': 0.2, 'C': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Tuning for Parameter C.\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "svr_regressor = SVR(verbose=True)\n",
    "\n",
    "svr_cols = ['weather', 'temp', 'atemp', 'humidity', 'windspeed',\n",
    "            'holiday', 'workingday', 'season',\n",
    "            'hour', 'year', 'best_condition', 'not_fav']\n",
    "\n",
    "Xtrain_svr = mod_data[svr_cols]\n",
    "ytrain_svr = mod_data['count_log']\n",
    "\n",
    "regre_svr = SVR(verbose=True)\n",
    "C = [float(x) for x in np.linspace(start = 0.5, stop = 2, num = 5)]\n",
    "kernelstring = ['rbf']\n",
    "gamma = ['auto']\n",
    "epsilon = [0.2]\n",
    "\n",
    "# create random grid\n",
    "random_grid = {'C': C, 'kernel': kernelstring, 'gamma': gamma, 'epsilon': epsilon}\n",
    "\n",
    "# Random search of parameters\n",
    "svr_random = RandomizedSearchCV(estimator = svr_regressor, param_distributions = random_grid, n_iter = 10, cv = 5, verbose=2, random_state=123, n_jobs = -1)\n",
    "\n",
    "# Fit the model\n",
    "svr_random.fit(Xtrain_svr, np.array(ytrain_svr))\n",
    "\n",
    "# print results\n",
    "print(svr_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n",
      "203\n",
      "284\n",
      "264\n",
      "288\n",
      "264\n",
      "288\n",
      "275\n",
      "264\n",
      "288\n",
      "263\n",
      "285\n",
      "288\n",
      "237\n",
      "288\n",
      "264\n",
      "288\n",
      "264\n",
      "288\n",
      "288\n",
      "264\n",
      "252\n",
      "263\n",
      "286\n"
     ]
    }
   ],
   "source": [
    "# Predicting with tuned parameters of SVR\n",
    "svr_cols = ['weather', 'temp', 'atemp', 'humidity', 'windspeed',\n",
    "            'holiday', 'workingday', 'season',\n",
    "            'hour', 'year', 'best_condition', 'not_fav']\n",
    "\n",
    "clf_svr = SVR(kernel= 'rbf', gamma = 'auto', epsilon = 0.2, C = 0.5)\n",
    "\n",
    "df_test_svr = pd.read_csv(r\"D:\\OneDrive - Queen's University\\ECE\\Statistical Learning\\Midterm\\test.csv\")\n",
    "mod_svr = predict_on_test_set_2(mod_data, clf_svr, svr_cols)\n",
    "\n",
    "\n",
    "svr_y_pred = np.round(mod_svr)\n",
    "\n",
    "# output predictions for submission\n",
    "\n",
    "df_test_svr['count'] = svr_y_pred\n",
    "final_df = df_test_svr[['datetime', 'count']].copy()\n",
    "final_df.to_csv('submit14.csv', index=False) # kaggle score 0.98756"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with one hot encoding the categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>peak_time</th>\n",
       "      <th>weekoff_count</th>\n",
       "      <th>best_condition</th>\n",
       "      <th>not_fav</th>\n",
       "      <th>sine_hr</th>\n",
       "      <th>cos_hr</th>\n",
       "      <th>casual_log</th>\n",
       "      <th>registered_log</th>\n",
       "      <th>count_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.40</td>\n",
       "      <td>81</td>\n",
       "      <td>6.986063</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>2.833213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.63</td>\n",
       "      <td>80</td>\n",
       "      <td>6.845595</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>3.496508</td>\n",
       "      <td>3.713572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.63</td>\n",
       "      <td>80</td>\n",
       "      <td>6.845595</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>3.332205</td>\n",
       "      <td>3.496508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.40</td>\n",
       "      <td>75</td>\n",
       "      <td>6.672821</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>2.639057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.40</td>\n",
       "      <td>75</td>\n",
       "      <td>6.672821</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17374</td>\n",
       "      <td>2012-12-31 19:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10.66</td>\n",
       "      <td>12.88</td>\n",
       "      <td>60</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17375</td>\n",
       "      <td>2012-12-31 20:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10.66</td>\n",
       "      <td>12.88</td>\n",
       "      <td>60</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17376</td>\n",
       "      <td>2012-12-31 21:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>12.88</td>\n",
       "      <td>60</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17377</td>\n",
       "      <td>2012-12-31 22:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>13.63</td>\n",
       "      <td>56</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17378</td>\n",
       "      <td>2012-12-31 23:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>13.63</td>\n",
       "      <td>65</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17379 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  datetime  season  holiday  workingday  weather   temp  \\\n",
       "0      2011-01-01 00:00:00       1        0           0        1   9.84   \n",
       "1      2011-01-01 01:00:00       1        0           0        1   9.02   \n",
       "2      2011-01-01 02:00:00       1        0           0        1   9.02   \n",
       "3      2011-01-01 03:00:00       1        0           0        1   9.84   \n",
       "4      2011-01-01 04:00:00       1        0           0        1   9.84   \n",
       "...                    ...     ...      ...         ...      ...    ...   \n",
       "17374  2012-12-31 19:00:00       1        0           1        2  10.66   \n",
       "17375  2012-12-31 20:00:00       1        0           1        2  10.66   \n",
       "17376  2012-12-31 21:00:00       1        0           1        1  10.66   \n",
       "17377  2012-12-31 22:00:00       1        0           1        1  10.66   \n",
       "17378  2012-12-31 23:00:00       1        0           1        1  10.66   \n",
       "\n",
       "       atemp  humidity  windspeed  casual  ...  year  peak_time weekoff_count  \\\n",
       "0      14.40        81   6.986063       3  ...  2011          0             0   \n",
       "1      13.63        80   6.845595       8  ...  2011          0             0   \n",
       "2      13.63        80   6.845595       5  ...  2011          0             0   \n",
       "3      14.40        75   6.672821       3  ...  2011          0             0   \n",
       "4      14.40        75   6.672821       0  ...  2011          0             0   \n",
       "...      ...       ...        ...     ...  ...   ...        ...           ...   \n",
       "17374  12.88        60  11.000000       0  ...  2012          1             0   \n",
       "17375  12.88        60  11.000000       0  ...  2012          0             0   \n",
       "17376  12.88        60  11.000000       0  ...  2012          0             0   \n",
       "17377  13.63        56   9.000000       0  ...  2012          0             0   \n",
       "17378  13.63        65   9.000000       0  ...  2012          0             0   \n",
       "\n",
       "       best_condition  not_fav   sine_hr    cos_hr  casual_log  \\\n",
       "0                   0        0  0.000000  1.000000    1.386294   \n",
       "1                   0        0  0.258819  0.965926    2.197225   \n",
       "2                   0        0  0.500000  0.866025    1.791759   \n",
       "3                   0        0  0.707107  0.707107    1.386294   \n",
       "4                   0        0  0.866025  0.500000    0.000000   \n",
       "...               ...      ...       ...       ...         ...   \n",
       "17374               0        1 -0.965926  0.258819    0.000000   \n",
       "17375               0        1 -0.866025  0.500000    0.000000   \n",
       "17376               0        1 -0.707107  0.707107    0.000000   \n",
       "17377               0        0 -0.500000  0.866025    0.000000   \n",
       "17378               0        1 -0.258819  0.965926    0.000000   \n",
       "\n",
       "       registered_log  count_log  \n",
       "0            2.639057   2.833213  \n",
       "1            3.496508   3.713572  \n",
       "2            3.332205   3.496508  \n",
       "3            2.397895   2.639057  \n",
       "4            0.693147   0.693147  \n",
       "...               ...        ...  \n",
       "17374        0.000000   0.000000  \n",
       "17375        0.000000   0.000000  \n",
       "17376        0.000000   0.000000  \n",
       "17377        0.000000   0.000000  \n",
       "17378        0.000000   0.000000  \n",
       "\n",
       "[17379 rows x 27 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_data = mod_data.copy()\n",
    "one_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'season', 'holiday', 'workingday', 'weather', 'temp',\n",
       "       'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count',\n",
       "       'date', 'hour', 'month', 'weekday', 'day', 'year', 'peak_time',\n",
       "       'weekoff_count', 'best_condition', 'not_fav', 'sine_hr', 'cos_hr',\n",
       "       'casual_log', 'registered_log', 'count_log'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_encode(data, column):\n",
    "    df = pd.get_dummies(data, columns=column, prefix=[\"season_is\", \"holiday_is\", \"workingday_is\", \"hour_is\"])\n",
    "    return df\n",
    "\n",
    "one_data_1 = dummy_encode(one_data[['season', 'holiday', 'workingday', 'hour']], ['season', 'holiday', 'workingday', 'hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_data = one_data.join(one_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>...</th>\n",
       "      <th>hour_is_14</th>\n",
       "      <th>hour_is_15</th>\n",
       "      <th>hour_is_16</th>\n",
       "      <th>hour_is_17</th>\n",
       "      <th>hour_is_18</th>\n",
       "      <th>hour_is_19</th>\n",
       "      <th>hour_is_20</th>\n",
       "      <th>hour_is_21</th>\n",
       "      <th>hour_is_22</th>\n",
       "      <th>hour_is_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.40</td>\n",
       "      <td>81</td>\n",
       "      <td>6.986063</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.63</td>\n",
       "      <td>80</td>\n",
       "      <td>6.845595</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.63</td>\n",
       "      <td>80</td>\n",
       "      <td>6.845595</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.40</td>\n",
       "      <td>75</td>\n",
       "      <td>6.672821</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.40</td>\n",
       "      <td>75</td>\n",
       "      <td>6.672821</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-01 05:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9.84</td>\n",
       "      <td>12.88</td>\n",
       "      <td>75</td>\n",
       "      <td>6.003906</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2011-01-01 06:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.63</td>\n",
       "      <td>80</td>\n",
       "      <td>6.845595</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2011-01-01 07:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.20</td>\n",
       "      <td>12.88</td>\n",
       "      <td>86</td>\n",
       "      <td>6.762674</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2011-01-01 08:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.40</td>\n",
       "      <td>75</td>\n",
       "      <td>6.672821</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2011-01-01 09:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.12</td>\n",
       "      <td>17.42</td>\n",
       "      <td>76</td>\n",
       "      <td>9.359136</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  season  holiday  workingday  weather   temp  atemp  \\\n",
       "0  2011-01-01 00:00:00       1        0           0        1   9.84  14.40   \n",
       "1  2011-01-01 01:00:00       1        0           0        1   9.02  13.63   \n",
       "2  2011-01-01 02:00:00       1        0           0        1   9.02  13.63   \n",
       "3  2011-01-01 03:00:00       1        0           0        1   9.84  14.40   \n",
       "4  2011-01-01 04:00:00       1        0           0        1   9.84  14.40   \n",
       "5  2011-01-01 05:00:00       1        0           0        2   9.84  12.88   \n",
       "6  2011-01-01 06:00:00       1        0           0        1   9.02  13.63   \n",
       "7  2011-01-01 07:00:00       1        0           0        1   8.20  12.88   \n",
       "8  2011-01-01 08:00:00       1        0           0        1   9.84  14.40   \n",
       "9  2011-01-01 09:00:00       1        0           0        1  13.12  17.42   \n",
       "\n",
       "   humidity  windspeed  casual  ...  hour_is_14  hour_is_15 hour_is_16  \\\n",
       "0        81   6.986063       3  ...           0           0          0   \n",
       "1        80   6.845595       8  ...           0           0          0   \n",
       "2        80   6.845595       5  ...           0           0          0   \n",
       "3        75   6.672821       3  ...           0           0          0   \n",
       "4        75   6.672821       0  ...           0           0          0   \n",
       "5        75   6.003906       0  ...           0           0          0   \n",
       "6        80   6.845595       2  ...           0           0          0   \n",
       "7        86   6.762674       1  ...           0           0          0   \n",
       "8        75   6.672821       1  ...           0           0          0   \n",
       "9        76   9.359136       8  ...           0           0          0   \n",
       "\n",
       "   hour_is_17  hour_is_18 hour_is_19  hour_is_20  hour_is_21  hour_is_22  \\\n",
       "0           0           0          0           0           0           0   \n",
       "1           0           0          0           0           0           0   \n",
       "2           0           0          0           0           0           0   \n",
       "3           0           0          0           0           0           0   \n",
       "4           0           0          0           0           0           0   \n",
       "5           0           0          0           0           0           0   \n",
       "6           0           0          0           0           0           0   \n",
       "7           0           0          0           0           0           0   \n",
       "8           0           0          0           0           0           0   \n",
       "9           0           0          0           0           0           0   \n",
       "\n",
       "   hour_is_23  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "5           0  \n",
       "6           0  \n",
       "7           0  \n",
       "8           0  \n",
       "9           0  \n",
       "\n",
       "[10 rows x 59 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test_set_2(data_, model, x_cols):\n",
    "    # prepare training set\n",
    "    df_ = data_[data_['count'] != 0].copy()\n",
    "    df_t = data_[data_['count'] == 0].copy()\n",
    "    years = [2011, 2012]\n",
    "    df_batch = pd.DataFrame()\n",
    "    df_result = pd.DataFrame()\n",
    "    result = np.empty((0,1), float)\n",
    "    y_result = []\n",
    "    for yr in years:\n",
    "        for i in range(1, 13):\n",
    "            df_train = df_[(df_['month'] == i) & (df_['year'] == yr)]\n",
    "            df_batch = df_batch.append(df_train)\n",
    "            \n",
    "            X_train = df_batch[x_cols]\n",
    "            y_train_cas = np.array(df_batch['casual_log'])\n",
    "            y_train_reg = np.array(df_batch['registered_log'])\n",
    "\n",
    "            # prepare test set\n",
    "            df_test = df_t[(df_t['month'] == i) & (df_t['year'] == yr)]\n",
    "            X_test = df_test[x_cols]\n",
    "\n",
    "            casual_model = model.fit(X_train, y_train_cas)\n",
    "            y_pred_cas = casual_model.predict(X_test)\n",
    "            y_pred_cas = np.exp(y_pred_cas) - 1\n",
    "            registered_model = model.fit(X_train, y_train_reg)\n",
    "            y_pred_reg = registered_model.predict(X_test)\n",
    "            y_pred_reg = np.exp(y_pred_reg) - 1\n",
    "            y_count = y_pred_cas + y_pred_reg\n",
    "            print(y_count.size)\n",
    "            y_result = np.concatenate([y_result, y_count])\n",
    "    return y_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'season', 'holiday', 'workingday', 'weather', 'temp',\n",
       "       'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count',\n",
       "       'date', 'hour', 'month', 'weekday', 'year', 'peak_time',\n",
       "       'weekoff_count', 'best_condition', 'not_fav', 'sine_hr', 'cos_hr',\n",
       "       'casual_log', 'registered_log', 'count_log', 'day', 'season_is_1',\n",
       "       'season_is_2', 'season_is_3', 'season_is_4', 'holiday_is_0',\n",
       "       'holiday_is_1', 'workingday_is_0', 'workingday_is_1', 'hour_is_0',\n",
       "       'hour_is_1', 'hour_is_2', 'hour_is_3', 'hour_is_4', 'hour_is_5',\n",
       "       'hour_is_6', 'hour_is_7', 'hour_is_8', 'hour_is_9', 'hour_is_10',\n",
       "       'hour_is_11', 'hour_is_12', 'hour_is_13', 'hour_is_14', 'hour_is_15',\n",
       "       'hour_is_16', 'hour_is_17', 'hour_is_18', 'hour_is_19', 'hour_is_20',\n",
       "       'hour_is_21', 'hour_is_22', 'hour_is_23'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n",
      "203\n",
      "284\n",
      "264\n",
      "288\n",
      "264\n",
      "288\n",
      "275\n",
      "264\n",
      "288\n",
      "263\n",
      "285\n",
      "288\n",
      "237\n",
      "288\n",
      "264\n",
      "288\n",
      "264\n",
      "288\n",
      "288\n",
      "264\n",
      "252\n",
      "263\n",
      "286\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': 1500, 'max_depth': 15, 'random_state': 123, 'min_samples_split' : 14, 'n_jobs': -1}\n",
    "rf_model = RandomForestRegressor(**params)\n",
    "\n",
    "rf_cols = ['season_is_1', 'season_is_2', 'season_is_3', 'season_is_4','holiday_is_0',\n",
    "           'holiday_is_1', 'workingday_is_0', 'workingday_is_1', 'weather', 'temp',\n",
    "           'atemp', 'humidity', 'windspeed', 'peak_time', 'not_fav', 'sine_hr', 'cos_hr']\n",
    "\n",
    "rnd_pred = predict_on_test_set_2(one_data, rf_model, rf_cols)\n",
    "\n",
    "rf_y_pred = np.round(rnd_pred)\n",
    "\n",
    "# output predictions for submission\n",
    "\n",
    "df_test['count'] = rf_y_pred\n",
    "final_df = df_test[['datetime', 'count']].copy()\n",
    "final_df.to_csv('submit11.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n",
      "203\n",
      "284\n",
      "264\n",
      "288\n",
      "264\n",
      "288\n",
      "275\n",
      "264\n",
      "288\n",
      "263\n",
      "285\n",
      "288\n",
      "237\n",
      "288\n",
      "264\n",
      "288\n",
      "264\n",
      "288\n",
      "288\n",
      "264\n",
      "252\n",
      "263\n",
      "286\n"
     ]
    }
   ],
   "source": [
    "gbm_cols = [\n",
    "    'weather', 'temp', 'atemp', 'humidity', 'windspeed',\n",
    "    'holiday_is_0', 'holiday_is_1', 'workingday_is_0', \n",
    "    'workingday_is_1', 'season_is_1', 'season_is_2', 'season_is_3', 'season_is_4',\n",
    "    'sine_hr', 'cos_hr', 'year', 'best_condition']\n",
    "\n",
    "best_params = {'n_estimators': 500, 'min_samples_leaf': 8, 'learning_rate': 0.1, 'subsample': 0.7, 'random_state': 123}\n",
    "\n",
    "gbm_model = GradientBoostingRegressor(**best_params)\n",
    "\n",
    "gbm_pred = predict_on_test_set_2(one_data, gbm_model, gbm_cols)\n",
    "\n",
    "y_pred = np.round(gbm_pred)\n",
    "\n",
    "# output predictions for submission\n",
    "\n",
    "df_test['count'] = y_pred\n",
    "final_df = df_test[['datetime', 'count']].copy()\n",
    "final_df.to_csv('submit12.csv', index=False) # kaggle score (one hot encoding) 0.42257"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Cross Validation with Batch ## doesn't work! memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  4.8min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 2000, 'max_features': 'auto', 'max_depth': None}\n",
      "2\n",
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   31.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  6.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1111 out of 1111 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1111, 'max_features': 'auto', 'max_depth': None}\n",
      "3\n",
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   46.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  3.5min\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 10.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1777 out of 1777 | elapsed:    6.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1777, 'max_features': 'auto', 'max_depth': 36}\n",
      "4\n",
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   53.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  4.5min\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 13.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1222 out of 1222 | elapsed:    7.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1222, 'max_features': 'auto', 'max_depth': 41}\n",
      "5\n",
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 15.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1666 out of 1666 | elapsed:   10.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1666, 'max_features': 'auto', 'max_depth': 27}\n",
      "6\n",
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "could not allocate 229376 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 516, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\", line 330, in fit\n    for i, t in enumerate(trees))\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 924, in __call__\n    while self.dispatch_one_batch(iterator):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n    result = ImmediateResult(func)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n    self.results = batch()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\", line 118, in _parallel_build_trees\n    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\", line 1157, in fit\n    X_idx_sorted=X_idx_sorted)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\", line 380, in fit\n    builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)\n  File \"sklearn\\tree\\_tree.pyx\", line 145, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n  File \"sklearn\\tree\\_tree.pyx\", line 243, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n  File \"sklearn\\tree\\_tree.pyx\", line 740, in sklearn.tree._tree.Tree._add_node\n  File \"sklearn\\tree\\_tree.pyx\", line 711, in sklearn.tree._tree.Tree._resize_c\n  File \"sklearn\\tree\\_utils.pyx\", line 41, in sklearn.tree._utils.safe_realloc\nMemoryError: could not allocate 229376 bytes\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-269-535c9b9ae107>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mcnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mcount_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrfc_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mbest_params_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    686\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1467\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[0;32m   1468\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1469\u001b[1;33m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    665\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 667\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    935\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    433\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: could not allocate 229376 bytes"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rand_clas_1 = RandomForestRegressor(verbose=True, random_state=123)\n",
    "\n",
    "# number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 1000, stop = 2000, num = 10)]\n",
    "\n",
    "# number of features at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# max depth\n",
    "max_depth = [int(x) for x in np.linspace(10, 50, num = 10)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# create random grid\n",
    "random_grid = {\n",
    " 'n_estimators': n_estimators,\n",
    " 'max_features': max_features,\n",
    " 'max_depth': max_depth\n",
    " }\n",
    "\n",
    "# Random search of parameters\n",
    "rfc_random = RandomizedSearchCV(estimator = rand_clas_1, param_distributions = random_grid, n_iter = 50, cv = 10, verbose=1, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the model\n",
    "df_ = one_data[one_data['count'] != 0].copy()\n",
    "years = [2011, 2012]\n",
    "df_batch = pd.DataFrame()\n",
    "best_params_cv = []\n",
    "cnt = 1\n",
    "\n",
    "for yr in years:\n",
    "    for i in range(1, 13):\n",
    "        #prepare train set with batches\n",
    "        df_train = df_[(df_['month'] == i) & (df_['year'] == yr)]\n",
    "        df_batch = df_batch.append(df_train)\n",
    "        X_train = df_batch[x_cols]\n",
    "        y_train = np.array(df_batch['count_log'])\n",
    "        print(cnt)\n",
    "        cnt = cnt + 1\n",
    "        count_model = rfc_random.fit(X_train, y_train)\n",
    "        print(count_model.best_params_)\n",
    "        best_params_cv.append(count_model.best_params_)\n",
    "print(best_params_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   19.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:  1.8min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1800, 'max_features': 'auto', 'max_depth': 18}\n",
      "2\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:  2.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1800, 'max_features': 'auto', 'max_depth': None}\n",
      "3\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   29.8s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:  3.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1000, 'max_features': 'auto', 'max_depth': 18}\n",
      "4\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   38.4s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:  4.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1500 out of 1500 | elapsed:    7.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1500, 'max_features': 'auto', 'max_depth': None}\n",
      "5\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   42.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:  5.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    8.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1200, 'max_features': 'auto', 'max_depth': 25}\n",
      "6\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   53.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:  6.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   14.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1800, 'max_features': 'auto', 'max_depth': None}\n",
      "7\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:  7.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    9.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1000, 'max_features': 'auto', 'max_depth': 20}\n",
      "8\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:  8.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   17.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1800, 'max_features': 'auto', 'max_depth': 15}\n",
      "9\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.3min\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:  9.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   10.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1000, 'max_features': 'auto', 'max_depth': 15}\n",
      "10\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 10.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   14.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1000, 'max_features': 'auto', 'max_depth': 25}\n",
      "11\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 11.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   27.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1800, 'max_features': 'auto', 'max_depth': 18}\n",
      "12\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 12.9min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   33.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1800, 'max_features': 'auto', 'max_depth': 20}\n",
      "13\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 13.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   33.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1800, 'max_features': 'auto', 'max_depth': 20}\n",
      "14\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 14.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   19.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1000, 'max_features': 'auto', 'max_depth': 18}\n",
      "15\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  8.7min\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "could not allocate 458752 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 516, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\", line 330, in fit\n    for i, t in enumerate(trees))\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 924, in __call__\n    while self.dispatch_one_batch(iterator):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n    result = ImmediateResult(func)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n    self.results = batch()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\", line 118, in _parallel_build_trees\n    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\", line 1157, in fit\n    X_idx_sorted=X_idx_sorted)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\", line 380, in fit\n    builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)\n  File \"sklearn\\tree\\_tree.pyx\", line 145, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n  File \"sklearn\\tree\\_tree.pyx\", line 243, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n  File \"sklearn\\tree\\_tree.pyx\", line 740, in sklearn.tree._tree.Tree._add_node\n  File \"sklearn\\tree\\_tree.pyx\", line 711, in sklearn.tree._tree.Tree._resize_c\n  File \"sklearn\\tree\\_utils.pyx\", line 41, in sklearn.tree._utils.safe_realloc\nMemoryError: could not allocate 458752 bytes\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-277-9971adea6955>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mcnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mcount_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrfc_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mbest_params_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    686\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1467\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[0;32m   1468\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1469\u001b[1;33m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    665\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 667\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    935\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    433\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: could not allocate 458752 bytes"
     ]
    }
   ],
   "source": [
    "# fine tuned \n",
    "#still doesn't work. Need more computational power to perform grid search cv with batches.\n",
    "rand_clas_1 = RandomForestRegressor(verbose=True, random_state=123)\n",
    "# params = {'n_estimators': 1500, 'max_depth': 15, 'random_state': 123, 'min_samples_split' : 14, 'n_jobs': -1}\n",
    "# number of trees in random forest\n",
    "n_estimators = [1000, 1200, 1500, 1800]\n",
    "# number of features at every split\n",
    "min_samples_split = [12, 14, 16, 18]\n",
    "# max depth\n",
    "max_depth = [10, 12, 15, 18, 20, 25]\n",
    "max_depth.append(None)\n",
    "# create random grid\n",
    "\n",
    "random_grid = {\n",
    " 'n_estimators': n_estimators,\n",
    " 'max_features': max_features,\n",
    " 'max_depth': max_depth\n",
    " }\n",
    "\n",
    "# Random search of parameters\n",
    "rfc_random = RandomizedSearchCV(estimator = rand_clas_1, param_distributions = random_grid, n_iter = 50, cv = 5, verbose=2, random_state=123, n_jobs = -1)\n",
    "\n",
    "# Fit the model\n",
    "df_ = one_data[one_data['count'] != 0].copy()\n",
    "years = [2011, 2012]\n",
    "df_batch = pd.DataFrame()\n",
    "best_params_cv = []\n",
    "cnt = 1\n",
    "\n",
    "for yr in years:\n",
    "    for i in range(1, 13):\n",
    "        #prepare train set with batches\n",
    "        df_train = df_[(df_['month'] == i) & (df_['year'] == yr)]\n",
    "        df_batch = df_batch.append(df_train)\n",
    "        X_train = df_batch[x_cols]\n",
    "        y_train = np.array(df_batch['count_log'])\n",
    "        print(cnt)\n",
    "        cnt = cnt + 1\n",
    "        count_model = rfc_random.fit(X_train, y_train)\n",
    "        print(count_model.best_params_)\n",
    "        best_params_cv.append(count_model.best_params_)\n",
    "        \n",
    "print(best_params_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for SVR #Memory ERROR!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "svr_cols = ['weather', 'temp', 'atemp', 'humidity', 'windspeed', 'month', 'year', 'peak_time',\n",
    "            'weekoff_count', 'best_condition', 'not_fav', 'season_is_1',\n",
    "            'season_is_2', 'season_is_3', 'season_is_4', 'holiday_is_0',\n",
    "            'holiday_is_1', 'workingday_is_0', 'workingday_is_1', 'hour_is_0',\n",
    "            'hour_is_1', 'hour_is_2', 'hour_is_3', 'hour_is_4', 'hour_is_5',\n",
    "            'hour_is_6', 'hour_is_7', 'hour_is_8', 'hour_is_9', 'hour_is_10',\n",
    "            'hour_is_11', 'hour_is_12', 'hour_is_13', 'hour_is_14', 'hour_is_15',\n",
    "            'hour_is_16', 'hour_is_17', 'hour_is_18', 'hour_is_19', 'hour_is_20',\n",
    "            'hour_is_21', 'hour_is_22', 'hour_is_23']\n",
    "\n",
    "regre_svr = SVR(verbose=True)\n",
    "C = [float(x) for x in np.linspace(start = 0.5, stop = 2, num = 5)]\n",
    "degree = [3, 4, 5, 6]\n",
    "kernelstring = ['rbf', 'poly']\n",
    "gamma = ['auto']\n",
    "epsilon = [0.2, 0.3, 0.4]\n",
    "\n",
    "# create random grid\n",
    "random_grid = {'C': C, 'degree': degree, 'kernel': kernelstring, 'gamma': gamma, 'epsilon': epsilon}\n",
    "\n",
    "# Random search of parameters\n",
    "svr_random = RandomizedSearchCV(estimator = regre_svr, param_distributions = random_grid, cv = 10, verbose=1, random_state=123, n_jobs = -1)\n",
    "# Fit the model\n",
    "df_ = one_data[one_data['count'] != 0].copy()\n",
    "years = [2011, 2012]\n",
    "df_batch = pd.DataFrame()\n",
    "best_params_cv = []\n",
    "cnt = 1\n",
    "\n",
    "for yr in years:\n",
    "    for i in range(1, 13):\n",
    "        #prepare train set with batches\n",
    "        df_train = df_[(df_['month'] == i) & (df_['year'] == yr)]\n",
    "        df_batch = df_batch.append(df_train)\n",
    "        X_train = df_batch[svr_cols]\n",
    "        y_train = np.array(df_batch['count_log'])\n",
    "        print(cnt)\n",
    "        cnt = cnt + 1\n",
    "        count_model = svr_random.fit(X_train, y_train)\n",
    "        print(count_model.best_params_)\n",
    "        best_params_cv.append(count_model.best_params_)\n",
    "print(best_params_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So, I can conclude that though individual models did not do that well on the training datasets, but the best kaggle score was achieved by combining the predictions of the models. \n",
    "# The best Kaggle score I could achieve was - 0.41490, by combining the outputs of GradientBoostingRegressor (80%) and RandomForestRegressor (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
